{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from scipy.sparse import lil_matrix, coo_matrix\n",
    "from scipy.io import mmwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一つ上の階層のmoduleをインポートできるようにする\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append( str(current_dir) + '/../' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setting_param import MakeSample_link_prediction_new_InputDir as InputDir\n",
    "from setting_param import MakeSample_link_prediction_new_OutputDir as OutputDir\n",
    "from setting_param import Evaluation_prediction_num_of_node_new_LSTM_InputDir as predicted_num_InputDir\n",
    "\n",
    "from setting_param import Model_attribute_prediction_new_Baseline_OutputDir as Baseline_Out_InputDir\n",
    "from setting_param import Model_attribute_prediction_new_DeepMatchMax_OutputDir as DeepMatchMax_Out_InputDir\n",
    "from setting_param import Model_attribute_prediction_new_FNN_OutputDir as FNN_Out_InputDir\n",
    "\n",
    "from setting_param import MakeSample_link_prediction_new_Baseline_OutputDir as Baseline_OutputDir\n",
    "from setting_param import MakeSample_link_prediction_new_DeepMatchMax_OutputDir as DeepMatchMax_OutputDir\n",
    "from setting_param import MakeSample_link_prediction_new_FNN_OutputDir as FNN_OutputDir\n",
    "\n",
    "Method_list = [\"Baseline\", \"DeepMatchMax\", \"FNN\"]\n",
    "Out_InputDir_list = [Baseline_Out_InputDir, DeepMatchMax_Out_InputDir, FNN_Out_InputDir]\n",
    "OutputDir_list = [Baseline_OutputDir, DeepMatchMax_OutputDir, FNN_OutputDir]\n",
    "\n",
    "\n",
    "from setting_param import ratio_test\n",
    "from setting_param import ratio_valid\n",
    "from setting_param import L\n",
    "from setting_param import attribute_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(OutputDir)\n",
    "learning_type = [\"mix\", \"learning\", \"inference\"]\n",
    "for OutputDir_ in OutputDir_list:\n",
    "    os.mkdir(OutputDir_)\n",
    "    for l_type in learning_type:\n",
    "        os.mkdir(OutputDir_ + \"/\" + l_type)\n",
    "        os.mkdir(OutputDir_ + \"/\" + l_type + \"/input\")\n",
    "        os.mkdir(OutputDir_ + \"/\" + l_type + \"/label\")\n",
    "        os.mkdir(OutputDir_ + \"/\" + l_type + \"/mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3859, 68)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ EXIST_TABLE\n",
    "EXIST_TABLE = np.load(InputDir + '/exist_table.npy')\n",
    "EXIST_TABLE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node = EXIST_TABLE.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExistNodeList(ts):\n",
    "    assert ts >= 0, \"ts < 0 [referrence error]\"\n",
    "    return np.where(EXIST_TABLE[:, ts]==1)[0]\n",
    "\n",
    "def GetAppearedNodes(ts):\n",
    "    return set(ExistNodeList(ts)) - set(ExistNodeList(ts-1))\n",
    "\n",
    "def GetObservedNodes(ts, L):\n",
    "    U = set()\n",
    "    for i in range(L):\n",
    "        U |= set(ExistNodeList(ts-i))\n",
    "    return U\n",
    "\n",
    "def GetNodes(ts, L, node_type):\n",
    "    if node_type=='all':\n",
    "        node_set = set(ExistNodeList(ts))\n",
    "    elif node_type=='stay':\n",
    "        node_set = set(ExistNodeList(ts-1)) & set(ExistNodeList(ts))\n",
    "    elif node_type=='lost':\n",
    "        node_set = set(ExistNodeList(ts-1)) - set(ExistNodeList(ts))\n",
    "    elif node_type=='return':\n",
    "        node_set = GetAppearedNodes(ts) - (GetAppearedNodes(ts) - GetObservedNodes(ts-1, L))\n",
    "    elif node_type=='new':\n",
    "        node_set = GetAppearedNodes(ts) - GetObservedNodes(ts-1, L)\n",
    "        node_set |= GetNodes(ts, L, 'return')\n",
    "    return node_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nx(ts):\n",
    "    return  nx.from_numpy_matrix(np.load(InputDir + '/adjacency' + str(ts) + '.npy'))\n",
    "\n",
    "def SubNxNew(ts, L):\n",
    "    return nx.Graph(Nx(ts).edges(GetNodes(ts, L, 'new')))\n",
    "\n",
    "def SubNxLost(ts, L):\n",
    "    return nx.Graph(Nx(ts-1).edges(GetNodes(ts, L, 'lost')))\n",
    "\n",
    "def GetEdges(ts, L, edge_type):\n",
    "    G_1 = Nx(ts)\n",
    "    if edge_type == \"all\":\n",
    "        edge_set = G_1.edges\n",
    "    elif edge_type == 'stay':\n",
    "        G_0 = Nx(ts - 1)\n",
    "        edge_set = G_0.edges & G_1.edges\n",
    "    elif edge_type == \"appeared\":\n",
    "        G_0 = Nx(ts - 1)\n",
    "        edge_set = G_1.edges - G_0.edges - SubNxNew(ts, L).edges\n",
    "    elif edge_type == \"disappeared\":\n",
    "        G_0 = Nx(ts - 1)\n",
    "        edge_set = G_0.edges - G_1.edges - SubNxLost(ts, L).edges\n",
    "    return edge_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(ts, L, edge_type):\n",
    "    G = nx.Graph(list(GetEdges(ts, L, edge_type)))\n",
    "    A = np.array(nx.to_numpy_matrix(G, nodelist=[i for i in range(n_node)]))\n",
    "    return A\n",
    "\n",
    "def get_exist_matrix(ts):\n",
    "    index = np.where(EXIST_TABLE[:, ts] == 1)[0]\n",
    "    exist_row = np.zeros((n_node, n_node))\n",
    "    exist_row[index] = 1\n",
    "    exist_col = np.zeros((n_node, n_node))\n",
    "    exist_col[:, index] = 1\n",
    "    return exist_row * exist_col\n",
    "\n",
    "def NodeAttribute(ts):\n",
    "    return  np.load(InputDir + '/node_attribute' + str(ts) + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expanded_node_attribute_learning(ts, L, n_node, n_expanded):\n",
    "    node_attribute = NodeAttribute(ts)\n",
    "    node_attribute[sorted(GetNodes(ts, L, 'new'))] = 0\n",
    "    new_node_attribute = NodeAttribute(ts)[sorted(GetNodes(ts, L, 'new'))]\n",
    "    \n",
    "    expanded_attribute = np.zeros((n_node + n_expanded, NodeAttribute(ts).shape[1]))\n",
    "    expanded_attribute[:n_node] = node_attribute\n",
    "    expanded_attribute[n_node:n_node+new_node_attribute.shape[0]] = new_node_attribute\n",
    "    return expanded_attribute\n",
    "    \n",
    "def get_expanded_label_matrix_learning(ts, L, expanded_idx_dic, n_node, n_expanded):\n",
    "    expanded_edges = set()\n",
    "    for i, j in SubNxNew(ts, L).edges:\n",
    "        if i in expanded_idx_dic.keys():\n",
    "            i = expanded_idx_dic[i]\n",
    "        if j in expanded_idx_dic.keys():\n",
    "            j = expanded_idx_dic[j]\n",
    "        expanded_edges.add((i, j))\n",
    "    G = nx.Graph(list(expanded_edges))\n",
    "    A = np.array(nx.to_numpy_matrix(G, nodelist=[i for i in range(n_node +  n_expanded)]))\n",
    "    return A\n",
    "\n",
    "def get_expanded_mask_matrix_learning(ts, L, expanded_idx_dic, n_node, n_expanded):\n",
    "    expanded_matrix = np.zeros((n_node + n_expanded, n_node + n_expanded))\n",
    "    for n in GetNodes(ts, L, 'new'):\n",
    "        for s in GetNodes(ts, L, 'stay'):\n",
    "            expanded_matrix[expanded_idx_dic[n]][s] = 1\n",
    "            expanded_matrix[s][expanded_idx_dic[n]] = 1\n",
    "    return expanded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expanded_node_attribute_inference(ts, L, n_node, n_expanded, new):\n",
    "    node_attribute = NodeAttribute(ts)\n",
    "    node_attribute[sorted(GetNodes(ts, L, 'new'))] = 0\n",
    "    new_node_attribute = new\n",
    "    \n",
    "    expanded_attribute = np.zeros((n_node + n_expanded, NodeAttribute(ts).shape[1]))\n",
    "    expanded_attribute[:n_node] = node_attribute\n",
    "    expanded_attribute[n_node:n_node+new_node_attribute.shape[0]] = new_node_attribute\n",
    "    return expanded_attribute\n",
    "    \n",
    "def get_expanded_label_matrix_inference(ts, L, expanded_idx_dic, n_node, n_expanded):\n",
    "    expanded_edges = set()\n",
    "    for i, j in SubNxNew(ts, L).edges:\n",
    "        expanded_i = []\n",
    "        if i in expanded_idx_dic.keys():\n",
    "            for i_ in expanded_idx_dic[i]:\n",
    "                expanded_i.append(i_)\n",
    "        else:\n",
    "            expanded_i.append(i)\n",
    "        expanded_j = []\n",
    "        if j in expanded_idx_dic.keys():\n",
    "            for j_ in expanded_idx_dic[j]:\n",
    "                expanded_j.append(j_)\n",
    "        else:\n",
    "            expanded_j.append(j)\n",
    "        for i_ in expanded_i:\n",
    "            for j_ in expanded_j:\n",
    "                expanded_edges.add((i_, j_))\n",
    "    G = nx.Graph(list(expanded_edges))\n",
    "    A = np.array(nx.to_numpy_matrix(G, nodelist=[i for i in range(n_node +  n_expanded)]))\n",
    "    return A\n",
    "\n",
    "def get_expanded_mask_matrix_inference(ts, L, expanded_idx_dic, n_node, n_expanded):\n",
    "    expanded_matrix = np.zeros((n_node + n_expanded, n_node + n_expanded))\n",
    "    for n in GetNodes(ts, L, 'new'):\n",
    "        for s in GetNodes(ts, L, 'stay'):\n",
    "            if n in expanded_idx_dic.keys():\n",
    "                for n_ in expanded_idx_dic[n]:\n",
    "                    expanded_matrix[n_][s] = 1\n",
    "                    expanded_matrix[s][n_] = 1\n",
    "    return expanded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TsSplit(ts, L):\n",
    "    ts_train = [(ts+l) for l in range(L)]\n",
    "    ts_test = ts_train[-1]+1\n",
    "    ts_all = ts_train.copy()\n",
    "    ts_all.extend([ts_test])\n",
    "    return ts_train, ts_test, ts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paths_from_dir(dir_path):\n",
    "    # dir 以下のファイル名のリストを取得\n",
    "    path_list = glob.glob(dir_path + \"/*\")\n",
    "    # ソート (ゼロ埋めされていない数字の文字列のソート)\n",
    "    path_list = np.array(sorted(path_list, key=lambda s: int(re.findall(r'\\d+', s)[-1])))\n",
    "    return path_list\n",
    "\n",
    "def dev_test_split(all_idx, n_samples, ratio_test):\n",
    "    n_test = int(n_samples * ratio_test)\n",
    "    return all_idx[:-n_test], all_idx[-n_test:]\n",
    "\n",
    "def train_valid_split(dev_idx, n_samples, ratio_valid):\n",
    "    n_valid = int(n_samples * ratio_valid)\n",
    "    return dev_idx[:-n_valid], dev_idx[-n_valid:]\n",
    "\n",
    "def data_split(input_dir):\n",
    "    paths = load_paths_from_dir(input_dir + '/output')\n",
    "    new_num_ls = []\n",
    "    teacher_num_ls =[]\n",
    "    teacher_idx_ls =[]\n",
    "    new_ls = []\n",
    "    teacher_ls = []\n",
    "    node_pair_list_ls = []\n",
    "    for path in paths:\n",
    "        if 'new_num' in path.split('/')[-1]:\n",
    "            new_num_ls.append(path)\n",
    "        elif 'teacher_num' in path.split('/')[-1]:\n",
    "            teacher_num_ls.append(path)\n",
    "        elif 'teacher_idx' in path.split('/')[-1]:\n",
    "            teacher_idx_ls.append(path)\n",
    "        elif 'new' in path.split('/')[-1]:\n",
    "            new_ls.append(path)\n",
    "        elif 'teacher' in path.split('/')[-1]:\n",
    "            teacher_ls.append(path)\n",
    "        elif 'node_pair_list' in path.split('/')[-1]:\n",
    "            node_pair_list_ls.append(path)\n",
    "    return np.array(new_ls), np.array(teacher_ls), np.array(new_num_ls), np.array(teacher_num_ls), np.array(teacher_idx_ls), np.array(node_pair_list_ls)\n",
    "\n",
    "def load_npy_data(new_paths, teacher_paths, new_num_paths, teacher_num_paths, teacher_idx_paths, node_pair_list_paths, all_idx, ts):\n",
    "    idx = all_idx[ts-L]\n",
    "    new = np.load(new_paths[idx])\n",
    "    teacher = np.load(teacher_paths[idx])\n",
    "    new_num = np.load(new_num_paths[idx])\n",
    "    teacher_num = np.load(teacher_num_paths[idx])\n",
    "    teacher_idx = np.load(teacher_idx_paths[idx])\n",
    "    node_pair_list = np.load(node_pair_list_paths[idx])\n",
    "    return new, teacher, new_num, teacher_num, teacher_idx, node_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_new_node_numの最大値を取得\n",
    "predicted_new_node_num_list = []\n",
    "for ts in range(L, EXIST_TABLE.shape[1]-L):\n",
    "    predicted_new_node_num = int(np.load(predicted_num_InputDir + '/output/pred' + str(ts) + '.npy')[0])\n",
    "    predicted_new_node_num_list.append(predicted_new_node_num)\n",
    "max_predicted_new_node_num = max(predicted_new_node_num_list)\n",
    "max_predicted_new_node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_node_numの最大値を取得\n",
    "new_node_num_list = []\n",
    "for ts in range(L, EXIST_TABLE.shape[1]-L):\n",
    "    ts_train, ts_test, ts_all = TsSplit(ts, L)\n",
    "    new_node_num = len(GetNodes(ts_test, L, 'new'))\n",
    "    new_node_num_list.append(new_node_num)\n",
    "max_new_node_num = max(new_node_num_list)\n",
    "max_new_node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_expanded = max([max_predicted_new_node_num, max_new_node_num])\n",
    "n_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_idx, OutputDir in enumerate(OutputDir_list):\n",
    "    Model_Out_InputDir = Out_InputDir_list[method_idx]\n",
    "    \n",
    "    new_paths, teacher_paths, new_num_paths, teacher_num_paths, teacher_idx_paths, node_pair_list_paths = data_split(Model_Out_InputDir)\n",
    "    n_samples = len(new_paths)\n",
    "    all_idx = list(range(n_samples))\n",
    "    dev_idx, test_idx = dev_test_split(all_idx, n_samples, ratio_test)\n",
    "    train_idx, valid_idx = dev_test_split(dev_idx, n_samples, ratio_valid)\n",
    "    \n",
    "    for ts in range(L, EXIST_TABLE.shape[1]-L):\n",
    "        ts_train, ts_test, ts_all = TsSplit(ts, L)\n",
    "        new, teacher, new_num, teacher_num, teacher_idx, node_pair_list = load_npy_data(new_paths, teacher_paths, new_num_paths, teacher_num_paths, teacher_idx_paths, node_pair_list_paths, all_idx, ts)\n",
    "        \n",
    "        # reference check\n",
    "        assert sorted(GetNodes(ts_test, L, 'new')) == teacher_idx.tolist()[:teacher_num], 'reference error'\n",
    "        predicted_new_node_num = int(np.load(predicted_num_InputDir + '/output/pred' + str(ts) + '.npy')[0])\n",
    "        assert new_num == predicted_new_node_num, 'reference error'\n",
    "        new_node_num = len(GetNodes(ts_test, L, 'new'))\n",
    "        assert teacher_num == new_node_num, 'reference error'\n",
    "        assert new.shape[0] == max_predicted_new_node_num, 'reference error'\n",
    "        assert teacher.shape[0] == max_new_node_num, 'reference error'\n",
    "        \n",
    "        # 学習用    \n",
    "        # expanded_idx_dic = {teacher : expanded_new}\n",
    "        # expanded_new = n_node + teacher_idx\n",
    "        expanded_idx_dic = {}\n",
    "        for idx, n in enumerate(sorted(GetNodes(ts_test, L, 'new'))):\n",
    "            expanded_idx_dic[n] = n_node + idx\n",
    "        # input\n",
    "        node_attribute = get_expanded_node_attribute_learning(ts_test, L, n_node, n_expanded)        \n",
    "        # label\n",
    "        label = get_expanded_label_matrix_learning(ts_test, L, expanded_idx_dic, n_node, n_expanded)\n",
    "        # mask\n",
    "        mask = get_expanded_mask_matrix_learning(ts_test, L, expanded_idx_dic, n_node, n_expanded)\n",
    "        \n",
    "        label = lil_matrix(label)\n",
    "        mask = lil_matrix(mask)\n",
    "        \n",
    "        np.save(OutputDir + \"/learning/input/\" + str(ts), node_attribute)\n",
    "        mmwrite(OutputDir + \"/learning/label/\" + str(ts), label)\n",
    "        mmwrite(OutputDir + \"/learning/mask/\" + str(ts), mask)\n",
    "        # mix 用 学習時は正解データ\n",
    "        idx = all_idx[ts-L]\n",
    "        if idx in train_idx or idx in valid_idx:\n",
    "            np.save(OutputDir + \"/mix/input/\" + str(ts), node_attribute)\n",
    "            mmwrite(OutputDir + \"/mix/label/\" + str(ts), label)\n",
    "            mmwrite(OutputDir + \"/mix/mask/\" + str(ts), mask)\n",
    "        \n",
    "        # 推論用\n",
    "        # expanded_idx_dic = {teacher : list(expanded_new)}\n",
    "        # expanded_new = n_node + new_idx\n",
    "        expanded_idx_dic = defaultdict(list)\n",
    "        for new_row in range(new_num):\n",
    "            teacher_node = int(node_pair_list[new_row, 1])\n",
    "            new_node = int(node_pair_list[new_row, 0])\n",
    "            expanded_idx_dic[teacher_node].append(n_node + new_node)\n",
    "        # input\n",
    "        node_attribute = get_expanded_node_attribute_inference(ts_test, L, n_node, n_expanded, new)\n",
    "        # label\n",
    "        label = get_expanded_label_matrix_inference(ts_test, L, expanded_idx_dic, n_node, n_expanded)\n",
    "        # mask\n",
    "        mask = get_expanded_mask_matrix_inference(ts_test, L, expanded_idx_dic, n_node, n_expanded)\n",
    "\n",
    "        label = lil_matrix(label)\n",
    "        mask = lil_matrix(mask)\n",
    "        \n",
    "        np.save(OutputDir + \"/inference/input/\" + str(ts), node_attribute)\n",
    "        mmwrite(OutputDir + \"/inference/label/\" + str(ts), label)\n",
    "        mmwrite(OutputDir + \"/inference/mask/\" + str(ts), mask)\n",
    "        # mix 用 推論時は予測されたデータ\n",
    "        idx = all_idx[ts-L]\n",
    "        if idx in test_idx:\n",
    "            np.save(OutputDir + \"/mix/input/\" + str(ts), node_attribute)\n",
    "            mmwrite(OutputDir + \"/mix/label/\" + str(ts), label)\n",
    "            mmwrite(OutputDir + \"/mix/mask/\" + str(ts), mask)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
